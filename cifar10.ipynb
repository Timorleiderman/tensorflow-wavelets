{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Timorleiderman/MWCNN/blob/main/cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaN-or1PSqlc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b759e386-508a-400e-eb69-1959158e2750"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvMQYSLpvYQt"
      },
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/MWCNN')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKgXo17x3_hg"
      },
      "source": [
        "\n",
        "import pickle\n",
        "import datetime\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import models.cifar10CNN\n",
        "import models.WaveletCifar10CNN\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical, Sequence"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_9maP4z3_C8"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfvnzkdZSqli"
      },
      "source": [
        "weights_filepath = 'weights'\n",
        "logs_filepath = 'logs'\n",
        "\n",
        "\n",
        "if not os.path.exists(weights_filepath):\n",
        "    os.makedirs(weights_filepath)\n",
        "\n",
        "if not os.path.exists(logs_filepath):\n",
        "    os.makedirs(logs_filepath)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7Y0GE3jSqlj"
      },
      "source": [
        "nb_classes = 10\n",
        "num_folds = 10\n",
        "batch_size = 32\n",
        "epochs = 30\n",
        "\n",
        "lr = 1e-4  # learning rate\n",
        "beta_1 = 0.9         # beta 1 - for adam optimizer\n",
        "beta_2 = 0.96        # beta 2 - for adam optimizer\n",
        "epsilon = 1e-7        # epsilon - for adam optimizer\n",
        "\n",
        "trainFactor = 0.8\n",
        "imageShape = (32, 32, 3)  # CIFAR-10 60,000 32X32 color\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9Sur-BnSqlj"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc2eXRu6Sqlj"
      },
      "source": [
        "optimizer = Adam(learning_rate=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)  # SGD()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHTWJQwwSqlj"
      },
      "source": [
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxPcXofMSqlk"
      },
      "source": [
        "# # Define per-fold score containers <-- these are new\n",
        "# acc_per_fold = []\n",
        "# loss_per_fold = []\n",
        "\n",
        "# inputs = np.concatenate((x_train, x_test), axis=0)\n",
        "# targets = np.concatenate((y_train, y_test), axis=0)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwlTO1_uSqlk"
      },
      "source": [
        "# Define the K-fold Cross Validator\n",
        "# kfold = KFold(n_splits=num_folds, shuffle=True)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS6tyzZwSqlk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f924190-9aa6-451c-9d52-a0528ae34b38"
      },
      "source": [
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "# fold_no = 1\n",
        "# for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "# Create and Train the Model\n",
        "model = models.cifar10CNN.cifar10CNN(imageShape, nb_classes)\n",
        "history_file_path = \"trainHistoryCifar10CNN.txt\"  # save loss and val loss\n",
        "\n",
        "# model = models.WaveletCifar10CNN.WaveletCNN(imageShape, nb_classes)\n",
        "# history_file_path = \"trainHistoryWaveletCifar10CNN.txt\" # save loss and val loss\n",
        "# model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "# log_dir = logs_filepath + r\"/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "# h5_tmp = \"fold_\" + str(fold_no) + \"_tmp.h5\"\n",
        "h5_tmp = \"tmp.h5\"\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    validation_split=1 - trainFactor,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    verbose=2,\n",
        "                    callbacks=[\n",
        "                        ModelCheckpoint(h5_tmp, monitor='val_loss', verbose=0, save_best_only=True, mode='auto'),\n",
        "                        # ModelCheckpoint(h5_tmp, monitor='loss', verbose=0, save_best_only=True, mode='auto'),\n",
        "                        EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto')\n",
        "                        # EarlyStopping(monitor='loss', min_delta=1e-3, patience=10, verbose=1, mode='auto')\n",
        "                        # tensorboard_callback\n",
        "                        ],\n",
        "                    )\n",
        "\n",
        "# # Generate generalization metrics\n",
        "# scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "# print(\n",
        "#     f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1] * 100}%')\n",
        "# acc_per_fold.append(scores[1] * 100)\n",
        "# loss_per_fold.append(scores[0])\n",
        "\n",
        "# Increase fold number\n",
        "# fold_no = fold_no + 1\n",
        "\n",
        "model.load_weights(h5_tmp)\n",
        "# weights_path = os.path.join(weights_filepath, str(fold_no) + \"_WCNNN.h5\")\n",
        "weights_path = os.path.join(weights_filepath, \"WCNNN.h5\")\n",
        "model.save(weights_path)\n",
        "\n",
        "# Model Evaluation\n",
        "result = model.evaluate(x_test, y_test)\n",
        "\n",
        "with open(history_file_path, 'wb') as f:\n",
        "    pickle.dump(history.history, f)\n",
        "\n",
        "# # Generate a print\n",
        "# print('------------------------------------------------------------------------')\n",
        "# print(f'Training for fold {fold_no} ...')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIVyOPRnSqll"
      },
      "source": [
        "\n",
        "# # == Provide average scores ==\n",
        "# print('------------------------------------------------------------------------')\n",
        "# print('Score per fold')\n",
        "# for i in range(0, len(acc_per_fold)):\n",
        "#     print('------------------------------------------------------------------------')\n",
        "#     print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "# print('------------------------------------------------------------------------')\n",
        "# print('Average scores for all folds:')\n",
        "# print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "# print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "# print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR7psEeUSqll"
      },
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_file_path = r\"trainHistoryWaveletCifar10CNN.txt\"\n",
        "\n",
        "with open(history_file_path, 'rb') as pickle_file:\n",
        "    history = pickle.load(pickle_file)\n",
        "\n",
        "\n",
        "# plot train and validation loss\n",
        "plt.plot(history['loss'])\n",
        "plt.plot(history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}